{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex2_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM8xl/tzNzU4C354WGiMDPf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonmartz/RegressionTreeProject/blob/master/ex2_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHVs0sE587JW",
        "colab_type": "text"
      },
      "source": [
        "Check if GPU is enabled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15R3Mt5Q9b9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAv3sTcU9TWK",
        "colab_type": "text"
      },
      "source": [
        "Clone github project and install packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvEHaUqY_GwS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "a8b22ca7-e980-4e9c-ee50-9bf934c156a3"
      },
      "source": [
        "!git clone https://github.com/jonmartz/MachineLearningAss2\n",
        "!pip install category_encoders\n",
        "#!pip install shap"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MachineLearningAss2' already exists and is not an empty directory.\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba8sWJ2o_QJZ",
        "colab_type": "text"
      },
      "source": [
        "*Mount* google drive:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_P6WKCb_kE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94d738c3-718a-48b7-ff8d-aea774d482f8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-M5Ebdz_UMp",
        "colab_type": "text"
      },
      "source": [
        "Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79TmNqdn_uIy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1095524-240a-4b4b-b106-1df20a48c49d"
      },
      "source": [
        "# Import scikit-learn dataset library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import shap  # package used to calculate Shap values\n",
        "from sklearn import datasets\n",
        "# Import train_test_split function\n",
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from itertools import zip_longest\n",
        "from joblib import dump, load\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "# from sklearn.preprocessing import Imputer\n",
        "\n",
        "import xgboost as xgb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u92TfP9S_Pzy",
        "colab_type": "text"
      },
      "source": [
        "initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CReW7bpG_99V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_col = 'CLASS'\n",
        "\n",
        "parsed_train_set_encoded = []\n",
        "parsed_test_set_encoded = []\n",
        "dataset = []\n",
        "train_set = []\n",
        "train_set_encoded_a = []\n",
        "test_set = []\n",
        "test_set_encoded_b = []\n",
        "to_drop = []"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MFT-RPx_XFs",
        "colab_type": "text"
      },
      "source": [
        "Offline testing - in local environment, spolitting train set to smaller train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu0n6OMdARN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # =============================================================================\n",
        "# ##### OFFLINE testing code ####### train & validation\n",
        "# dataset = pd.read_csv(\"MachineLearningAss2/data/train.csv\")\n",
        "# train_fraction = 0.7\n",
        "# ## remove columns with minimal data\n",
        "# dataset =  rmissingvaluecol_onedf(dataset,80) #Here threshold is 10% which means we are going to drop columns having more than 10% of missing values\n",
        "# ## deal with NaN pre-encoding\n",
        "# #print(\"dataset pre impulation is:\", dataset )\n",
        "# strategy = 'most_frequent'\n",
        "# dataset = missing_values_impul(dataset, strategy)\n",
        "# #print(\"dataset post impulation is:\", dataset)\n",
        "# ## Drop correlated features\n",
        "# dataset = drop_corr_feat(dataset)\n",
        "\n",
        "# ## encoding\n",
        "# train_set, train_set_encoded_a, test_set, test_set_encoded_b = train_test_split(dataset, train_fraction)\n",
        "# #parsed_train_set_encoded = rmissingvaluecol_onedf(train_set_encoded_a,0.01) #Here threshold is 0.01% which means we are going to drop columns having more than 0.01% of missing values\n",
        "# #parsed_test_set_encoded = rmissingvaluecol_onedf(test_set_encoded_b,0.01) #Here threshold is 0.01% which means we are going to drop columns having more than 0.01% of missing values\n",
        "\n",
        "# ### removing columns to match train testing to actual testing\n",
        "# #parsed_train_set_encoded, parsed_test_set_encoded_pre = rmissingvaluecol_twodf(parsed_train_set_encoded, parsed_train_set_encoded_pre, 0.01)\n",
        "# #parsed_test_set_encoded, parsed_test_set_encoded_pre = rmissingvaluecol_twodf(parsed_test_set_encoded, parsed_test_set_encoded_pre, 0.01)\n",
        "\n",
        "\n",
        "# ## AR: write code to drop dependant columns, save them and set as parameter to online test\n",
        "\n",
        "\n",
        "# #print(\"\\n for offline training train_set_encoded_a:\\n\", train_set_encoded_a)\n",
        "# #print(\"\\n for offline training test_set_encoded_b:\\n\", test_set_encoded_b)\n",
        "# test(train_set_encoded_a, test_set_encoded_b, target_col, clf, \"train\")\n",
        "\n",
        "# # =============================================================================\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Sx8zZM_X-x",
        "colab_type": "text"
      },
      "source": [
        "##### ONLINE testing code ####### \n",
        "train_set and test_set are used to generate pred file for kaggle competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBjsOw9VA0rn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Reading train set file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX1XuAuuBBBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "695ebeb0-e3ff-47a2-febd-3582403ca012"
      },
      "source": [
        "dataset = pd.read_csv(\"MachineLearningAss2/data/train.csv\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Columns (1,2,4,5,7,8,9,10,11,13,14,15,17,18,19,20,23,24,26,27,28,33,34,35,36,37,38,39,40,44,45,46,47,48,49,64,70,71,78,80) have mixed types.Specify dtype option on import or set low_memory=False.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaFzvZIMA6Kt",
        "colab_type": "text"
      },
      "source": [
        "Removing columns with minimal info from train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmYOotShBdTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmissingvaluecol_onedf(dff, threshold):\n",
        "    my_df = dff.copy()\n",
        "    l = []\n",
        "    l = list(my_df.drop(my_df.loc[:, list((100 * (my_df.isnull().sum() / len(my_df.index)) >= threshold))].columns,\n",
        "                        1).columns.values)\n",
        "    #    print(\"# Columns having more than %s percent missing values:\"%threshold,(dff.shape[1] - len(l)))\n",
        "    #    print(\"Columns:\\n\",list(set(list((dff.columns.values))) - set(l)))\n",
        "    dff1 = my_df[l]\n",
        "    return (dff1)\n",
        "    # return l\n",
        "\n",
        "\n",
        "### if train_fraction = 1, just encode, no split\n",
        "train_fraction = 1\n",
        "## remove columns with minimal data\n",
        "dataset = rmissingvaluecol_onedf(dataset, 80)  # Here threshold is 80% which means we are going to drop columns having more than 80% of missing values\n",
        "## deal with NaN pre-encoding"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjtjBjibA6l2",
        "colab_type": "text"
      },
      "source": [
        "Imputing - dealing with missing values on train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC6_NRS8CMdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def missing_values_impul(dff, my_strategy):\n",
        "    my_df = dff.copy()\n",
        "    col = my_df.columns.values\n",
        "\n",
        "    my_df = my_df.fillna('')\n",
        "\n",
        "    imp = SimpleImputer(missing_values='', strategy=my_strategy)\n",
        "    my_df = imp.fit_transform(my_df)\n",
        "    my_df2 = pd.DataFrame(my_df, columns=col)\n",
        "    return my_df2\n",
        "\n",
        "\n",
        "strategy = 'most_frequent'\n",
        "dataset = missing_values_impul(dataset, strategy)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQy-GPfVA66s",
        "colab_type": "text"
      },
      "source": [
        "Dropping correlated features \n",
        "(Based on output during offline validation of drop_corr_feat)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iHaT4dtC3o0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "bf4aaee0-97a3-48e9-86a5-06ac198a8a62"
      },
      "source": [
        "def drop_corr_feat(dff):\n",
        "    my_df = dff.copy()\n",
        "    l_numeric_cols = []\n",
        "\n",
        "    ### AR: write for categorial value\n",
        "\n",
        "    ### for numeric values\n",
        "    for (columnName, columnData) in my_df.iteritems():\n",
        "        for value in columnData.values:\n",
        "            if not (type(value) is str):\n",
        "                l_numeric_cols.append(columnName)\n",
        "                break\n",
        "    numeric_df = my_df[l_numeric_cols]\n",
        "    for value in numeric_df.columns:\n",
        "        numeric_df[value] = numeric_df[value].astype(float)\n",
        "\n",
        "    corr_matrix = numeric_df.corr(method='pearson').abs()\n",
        "    print(\"corr matrix\", corr_matrix)\n",
        "    # Select upper triangle of correlation matrix\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "    # Find index of feature columns with correlation greater than 0.95\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "    print(\"to drop:\", to_drop)\n",
        "\n",
        "    my_df.drop(my_df[to_drop], axis=1)\n",
        "    \n",
        "    return my_df\n",
        "\n",
        "\n",
        "to_drop = ['A178', 'A182', 'A183', 'A189', 'A420', 'A422']\n",
        "dataset.drop(dataset[to_drop], axis=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A1</th>\n",
              "      <th>A4</th>\n",
              "      <th>A13</th>\n",
              "      <th>A17</th>\n",
              "      <th>A25</th>\n",
              "      <th>A26</th>\n",
              "      <th>A30</th>\n",
              "      <th>A31</th>\n",
              "      <th>A40</th>\n",
              "      <th>A42</th>\n",
              "      <th>A43</th>\n",
              "      <th>A44</th>\n",
              "      <th>A51</th>\n",
              "      <th>A52</th>\n",
              "      <th>A53</th>\n",
              "      <th>A54</th>\n",
              "      <th>A55</th>\n",
              "      <th>A57</th>\n",
              "      <th>A58</th>\n",
              "      <th>A59</th>\n",
              "      <th>A60</th>\n",
              "      <th>A61</th>\n",
              "      <th>A62</th>\n",
              "      <th>A63</th>\n",
              "      <th>A64</th>\n",
              "      <th>A67</th>\n",
              "      <th>A68</th>\n",
              "      <th>A69</th>\n",
              "      <th>A70</th>\n",
              "      <th>A74</th>\n",
              "      <th>A75</th>\n",
              "      <th>A76</th>\n",
              "      <th>A78</th>\n",
              "      <th>A79</th>\n",
              "      <th>A80</th>\n",
              "      <th>A84</th>\n",
              "      <th>A85</th>\n",
              "      <th>A87</th>\n",
              "      <th>A88</th>\n",
              "      <th>A89</th>\n",
              "      <th>...</th>\n",
              "      <th>A400</th>\n",
              "      <th>A401</th>\n",
              "      <th>A402</th>\n",
              "      <th>A403</th>\n",
              "      <th>A404</th>\n",
              "      <th>A405</th>\n",
              "      <th>A406</th>\n",
              "      <th>A407</th>\n",
              "      <th>A408</th>\n",
              "      <th>A409</th>\n",
              "      <th>A410</th>\n",
              "      <th>A411</th>\n",
              "      <th>A412</th>\n",
              "      <th>A413</th>\n",
              "      <th>A414</th>\n",
              "      <th>A415</th>\n",
              "      <th>A416</th>\n",
              "      <th>A417</th>\n",
              "      <th>A418</th>\n",
              "      <th>A419</th>\n",
              "      <th>A421</th>\n",
              "      <th>A423</th>\n",
              "      <th>A424</th>\n",
              "      <th>A425</th>\n",
              "      <th>A426</th>\n",
              "      <th>A436</th>\n",
              "      <th>A437</th>\n",
              "      <th>A438</th>\n",
              "      <th>A439</th>\n",
              "      <th>A440</th>\n",
              "      <th>A441</th>\n",
              "      <th>A442</th>\n",
              "      <th>A443</th>\n",
              "      <th>A444</th>\n",
              "      <th>A445</th>\n",
              "      <th>A448</th>\n",
              "      <th>A449</th>\n",
              "      <th>A451</th>\n",
              "      <th>A452</th>\n",
              "      <th>CLASS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>F</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>147</td>\n",
              "      <td>406</td>\n",
              "      <td>1134</td>\n",
              "      <td>742</td>\n",
              "      <td>497</td>\n",
              "      <td>210</td>\n",
              "      <td>25.9</td>\n",
              "      <td>83.3</td>\n",
              "      <td>27.3</td>\n",
              "      <td>25.55</td>\n",
              "      <td>16.8</td>\n",
              "      <td>83.3</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>F</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>147</td>\n",
              "      <td>406</td>\n",
              "      <td>1134</td>\n",
              "      <td>742</td>\n",
              "      <td>497</td>\n",
              "      <td>210</td>\n",
              "      <td>25.9</td>\n",
              "      <td>83.3</td>\n",
              "      <td>27.3</td>\n",
              "      <td>25.55</td>\n",
              "      <td>16.8</td>\n",
              "      <td>83.3</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>182</td>\n",
              "      <td>406</td>\n",
              "      <td>1134</td>\n",
              "      <td>742</td>\n",
              "      <td>497</td>\n",
              "      <td>140</td>\n",
              "      <td>23.1</td>\n",
              "      <td>77.7</td>\n",
              "      <td>57.4</td>\n",
              "      <td>25.27</td>\n",
              "      <td>4.2</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>182</td>\n",
              "      <td>406</td>\n",
              "      <td>1134</td>\n",
              "      <td>742</td>\n",
              "      <td>497</td>\n",
              "      <td>140</td>\n",
              "      <td>23.1</td>\n",
              "      <td>77.7</td>\n",
              "      <td>57.4</td>\n",
              "      <td>25.27</td>\n",
              "      <td>4.2</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>182</td>\n",
              "      <td>406</td>\n",
              "      <td>1134</td>\n",
              "      <td>742</td>\n",
              "      <td>497</td>\n",
              "      <td>140</td>\n",
              "      <td>23.1</td>\n",
              "      <td>77.7</td>\n",
              "      <td>57.4</td>\n",
              "      <td>25.27</td>\n",
              "      <td>4.2</td>\n",
              "      <td>7.7</td>\n",
              "      <td>0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10617</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>266</td>\n",
              "      <td>420</td>\n",
              "      <td>1141</td>\n",
              "      <td>665</td>\n",
              "      <td>462</td>\n",
              "      <td>42</td>\n",
              "      <td>4.2</td>\n",
              "      <td>44.8</td>\n",
              "      <td>31.5</td>\n",
              "      <td>9.59</td>\n",
              "      <td>0.7</td>\n",
              "      <td>9.1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.75</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10618</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>231</td>\n",
              "      <td>483</td>\n",
              "      <td>392</td>\n",
              "      <td>777</td>\n",
              "      <td>518</td>\n",
              "      <td>42</td>\n",
              "      <td>4.2</td>\n",
              "      <td>44.8</td>\n",
              "      <td>31.5</td>\n",
              "      <td>9.59</td>\n",
              "      <td>38.5</td>\n",
              "      <td>25.2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10619</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>231</td>\n",
              "      <td>483</td>\n",
              "      <td>392</td>\n",
              "      <td>777</td>\n",
              "      <td>518</td>\n",
              "      <td>42</td>\n",
              "      <td>4.2</td>\n",
              "      <td>44.8</td>\n",
              "      <td>31.5</td>\n",
              "      <td>9.59</td>\n",
              "      <td>38.5</td>\n",
              "      <td>25.2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10620</th>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>231</td>\n",
              "      <td>483</td>\n",
              "      <td>392</td>\n",
              "      <td>777</td>\n",
              "      <td>518</td>\n",
              "      <td>42</td>\n",
              "      <td>4.2</td>\n",
              "      <td>44.8</td>\n",
              "      <td>31.5</td>\n",
              "      <td>9.59</td>\n",
              "      <td>38.5</td>\n",
              "      <td>25.2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10621</th>\n",
              "      <td>O</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>Z</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>W</td>\n",
              "      <td>H</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>W</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>231</td>\n",
              "      <td>483</td>\n",
              "      <td>392</td>\n",
              "      <td>777</td>\n",
              "      <td>518</td>\n",
              "      <td>42</td>\n",
              "      <td>4.2</td>\n",
              "      <td>44.8</td>\n",
              "      <td>31.5</td>\n",
              "      <td>9.59</td>\n",
              "      <td>38.5</td>\n",
              "      <td>25.2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10622 rows  216 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      A1 A4 A13 A17 A25 A26 A30  ...  A444   A445  A448  A449 A451  A452 CLASS\n",
              "0      Z  Z   Z   Z   W   W   O  ...  27.3  25.55  16.8  83.3    0     7    No\n",
              "1      Z  Z   Z   Z   W   W   O  ...  27.3  25.55  16.8  83.3    0     7   Yes\n",
              "2      Z  Z   Z   Z   W   W   O  ...  57.4  25.27   4.2   7.7    0  5.25   Yes\n",
              "3      Z  Z   Z   O   W   W   O  ...  57.4  25.27   4.2   7.7    0  5.25   Yes\n",
              "4      Z  Z   Z   O   W   W   O  ...  57.4  25.27   4.2   7.7    0  5.25   Yes\n",
              "...   .. ..  ..  ..  ..  ..  ..  ...   ...    ...   ...   ...  ...   ...   ...\n",
              "10617  Z  Z   Z   Z   W   W   O  ...  31.5   9.59   0.7   9.1    0  1.75   Yes\n",
              "10618  Z  Z   Z   Z   W   W   O  ...  31.5   9.59  38.5  25.2    0     7    No\n",
              "10619  Z  Z   Z   Z   W   W   O  ...  31.5   9.59  38.5  25.2    0     7    No\n",
              "10620  Z  Z   Z   Z   W   W   O  ...  31.5   9.59  38.5  25.2    0     7    No\n",
              "10621  O  Z   Z   Z   W   W   O  ...  31.5   9.59  38.5  25.2    0     7    No\n",
              "\n",
              "[10622 rows x 216 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pIilXE3A2Ck",
        "colab_type": "text"
      },
      "source": [
        "Encoding the train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua0MEYSlEMOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(dataset, train_fraction, mode='train'):\n",
        "    \"\"\"\n",
        "    Splits the dataset into a train and a test set\n",
        "    :param dataset: data to be split\n",
        "    :param categorical_cols: list of the column names of the categorical columns (previously identified automatically)\n",
        "    :param train_fraction: portion of dataset to be used as train set\n",
        "    :return: a list [train set, one-hot-encoded train set, test set, one-hot-encoded test set]\n",
        "    \"\"\"\n",
        "\n",
        "    #### Default - set string values as categorial except target column   ####\n",
        "    ##########################################################################\n",
        "    categorical_cols = []\n",
        "    for (columnName, columnData) in dataset.iteritems():\n",
        "        for value in columnData.values:\n",
        "            if type(value) is str:\n",
        "                categorical_cols.append(columnName)\n",
        "                break\n",
        "\n",
        "    if (categorical_cols[-1] == \"CLASS\"):\n",
        "        categorical_cols = np.delete(categorical_cols, -1)\n",
        "\n",
        "    ## AR: improve categorial selction\n",
        "\n",
        "    dataset_encoded = OneHotEncoder(cols=categorical_cols, use_cat_names=True).fit_transform(dataset)\n",
        "\n",
        "    # I added these lines to make the dataset numerical and encode the label to 0 and 1\n",
        "    if mode == 'train':\n",
        "        for col in list(dataset_encoded.columns)[:-1]:\n",
        "            dataset_encoded[col] = pd.to_numeric(dataset_encoded[col])\n",
        "        dataset_encoded[target_col] = LabelEncoder().fit_transform(dataset_encoded[target_col])\n",
        "    else:\n",
        "        for col in list(dataset_encoded.columns):\n",
        "            dataset_encoded[col] = pd.to_numeric(dataset_encoded[col])\n",
        "\n",
        "    if (train_fraction == 1):\n",
        "        return dataset_encoded, dataset_encoded, dataset_encoded, dataset_encoded\n",
        "\n",
        "    train_len = int(len(dataset.index) * train_fraction)\n",
        "    train_set = dataset.sample(n=train_len, random_state=1)\n",
        "    train_set_encoded = dataset_encoded.loc[train_set.index].reset_index(drop=True)\n",
        "    test_set = dataset.drop(train_set.index).reset_index(drop=True)\n",
        "    test_set_encoded = dataset_encoded.drop(train_set.index).reset_index(drop=True)\n",
        "\n",
        "    return train_set.reset_index(drop=True), train_set_encoded, test_set, test_set_encoded\n",
        "    # return train_set.reset_index(drop=True), dataset_encoded, test_set, dataset_encoded\n",
        "\n",
        "\n",
        "\n",
        "## encoding\n",
        "train_set, train_set_encoded_a, test_set, test_set_encoded_b = train_test_split(dataset, train_fraction)\n",
        "actual_train_set = train_set_encoded_a"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abtQuGCzA5SG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Repeating the above steps for test set as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrg6qew7EePq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv(\"MachineLearningAss2/data/test.csv\")\n",
        "### if train_fraction = 1, just encode, no split\n",
        "train_fraction = 1\n",
        "## remove columns with minimal data\n",
        "dataset = rmissingvaluecol_onedf(dataset, 80)  # Here threshold is 10% which means we are going to drop columns having more than 10% of missing values\n",
        "## deal with NaN pre-encoding\n",
        "\n",
        "strategy = 'most_frequent'\n",
        "dataset = missing_values_impul(dataset, strategy)\n",
        "\n",
        "# Drop columns based on training set correlation\n",
        "to_drop = ['A178', 'A182', 'A183', 'A189', 'A420', 'A422']\n",
        "dataset.drop(dataset[to_drop], axis=1)\n",
        "\n",
        "## encoding\n",
        "train_set, train_set_encoded_a, test_set, test_set_encoded_b = train_test_split(dataset, train_fraction, mode='test')\n",
        "actual_test_set = train_set_encoded_a"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U1yeRDaA4Ku",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Match train set and test set to have similar common features (post proceesing) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdDfKLnyEu87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rmissingvaluecol_twodf(dff_train, dff_test):\n",
        "    my_train_df = dff_train.copy()\n",
        "    my_test_df = dff_test.copy()\n",
        "\n",
        "    l_removed_columns = []\n",
        "\n",
        "    for value in my_train_df.columns.values:\n",
        "        if value not in my_test_df.columns.values:\n",
        "            if (value != \"CLASS\"):\n",
        "                l_removed_columns.append(value)\n",
        "                my_train_df.drop(value, axis=1, inplace=True)\n",
        "            # my_train_df.drop(value)\n",
        "\n",
        "    for value in my_test_df.columns.values:\n",
        "        if value not in my_train_df.columns.values:\n",
        "            if (value != \"CLASS\"):\n",
        "                l_removed_columns.append(value)\n",
        "                my_test_df.drop(value, axis=1, inplace=True)\n",
        "\n",
        "    return my_train_df, my_test_df\n",
        "\n",
        "\n",
        "\n",
        "### removing columns to match train testing to actual testing\n",
        "actual_train_set, actual_test_set = rmissingvaluecol_twodf(actual_train_set, actual_test_set)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fX92PKlULN",
        "colab_type": "text"
      },
      "source": [
        "AUC plot function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLFUv2RKlaAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_auc_best (best_model, x_test, y_test_actual_auc):\n",
        "\n",
        "  print (\"\\nAUC plot of best result\\n\")\n",
        "        \n",
        "  dtrain_auc = xgb.DMatrix(x_test, label=y_test_actual_auc)\n",
        "  y_pred_test_auc = best_model.predict(dtrain_auc)\n",
        "  fpr, tpr, _ = roc_curve(y_test_actual_auc, y_pred_test_auc)  \n",
        "  plt.figure()\n",
        "  lw = 2\n",
        "  plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)')\n",
        "  plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('AUC of best result')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "\n",
        "  print(\"AUC in train set:\",roc_auc_score(y_test_actual_auc, y_pred_test_auc))\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T--8JUiZikz6",
        "colab_type": "text"
      },
      "source": [
        "SHAP values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtZpjfMUi-cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnHVgAzBUVQD",
        "colab_type": "text"
      },
      "source": [
        "Main Test - produce .out csv file for kaggle competition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmXRRV1yUZoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def test(parsed_train_set_encoded, parsed_test_set_encoded, target_col, algo, clf, mode):\n",
        "    x_train = parsed_train_set_encoded.drop(columns=target_col)\n",
        "    y_train = parsed_train_set_encoded[target_col]\n",
        "\n",
        "    \n",
        "    ## \"train\" mode was use for offline validation - the initial validation that splitted the train set \n",
        "    if (mode == \"train\"):\n",
        "        x_test = parsed_test_set_encoded.drop(columns=target_col)\n",
        "        y_test = parsed_test_set_encoded[target_col]\n",
        "        #print (\"x_train is:\", x_train)\n",
        "        #print (\"y_train is:\", y_train)\n",
        "        #clf.fit(x_train,y_train)\n",
        "        #dump(clf, r\"C:\\Users\\yrakotch\\Desktop\\tmp\\MasterDegree\\LemidaHishuvit\\exercise2\\train.joblib\") \n",
        "    elif (mode == \"test\"):\n",
        "        x_test = parsed_test_set_encoded\n",
        "        #print (\"test parsed data is: \", x_test)\n",
        "        #clf.fit(x_train,y_train)\n",
        "        #clf = load(r\"C:\\Users\\yrakotch\\Desktop\\tmp\\MasterDegree\\LemidaHishuvit\\exercise2\\train.joblib\") \n",
        "    \n",
        "    ### RANDOM FOREST \n",
        "    if (algo == \"RF\"):\n",
        "        print (\"RF start execution\")\n",
        "        clf.fit(x_train,y_train)\n",
        "        y_pred=clf.predict(x_test)\n",
        "        y_pred_proba=clf.predict_proba(x_test)\n",
        "        ### print AUC & accuracy on the \"test portion of the original train set\" (offline validation) \n",
        "        if (mode == \"train\"):\n",
        "            print(\"Accuracy of train (the test portion in train set):\",metrics.accuracy_score(y_test, y_pred))\n",
        "            print(\"AUC in train set:\",roc_auc_score(y_test, y_pred_proba[:,1]))\n",
        "        \n",
        "        ProbToYes = y_pred_proba[:,1] \n",
        "        print(\"ProbToYes is:\\n\",ProbToYes)\n",
        "        print (\"RF END execution\")\n",
        "    \n",
        "        with open(r\"C:\\Users\\yrakotch\\Desktop\\tmp\\MasterDegree\\LemidaHishuvit\\exercise2\\out.csv\", mode='w') as file:\n",
        "        #with open(r\"out.csv\", mode='w') as file:\n",
        "            writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "            writer.writerow([\"Id\", \"ProbToYes\"])\n",
        "            for i in range(len(ProbToYes)):\n",
        "                writer.writerow([int(i+1), ProbToYes[i]])\n",
        "\n",
        "    ### XGBOOST\n",
        "    elif (algo == \"XGB\"):\n",
        "        print (\"XGB execution\")\n",
        "        selected_dtrain = xgb.DMatrix(x_train, label=y_train)\n",
        "        #clf = xgb.XGBClassifier(seed=1,verbose_eval=True, n_jobs =-1, max_depth =7,learning_rate=0.3, reg_lambda =250, tree_method='gpu_hist')\n",
        "        xgb_param = clf.get_xgb_params()\n",
        "        print ('Start cross validation')\n",
        "        cvresult = xgb.cv(xgb_param, selected_dtrain, metrics=['auc'],num_boost_round=2000, early_stopping_rounds=5, stratified=True, seed=1301, verbose_eval=True, shuffle=True, nfold=4)\n",
        "        cvresult\n",
        "\n",
        "        #### Select best result out of all epochs\n",
        "\n",
        "        xgb_param = clf.get_xgb_params()\n",
        "        bst = xgb.train(xgb_param, selected_dtrain, num_boost_round=268, verbose_eval=True)\n",
        "        bst.save_model('my_exec.model')\n",
        "\n",
        "        ### Get test prediction  ####\n",
        "\n",
        "        x_test = actual_test_set[actual_train_set.columns[:-1]]\n",
        "        selected_dtest = xgb.DMatrix(x_test)\n",
        "        ypred = bst.predict(selected_dtest)\n",
        "        ypred\n",
        "        \n",
        "        ### Generate .csv out file for kaggle\n",
        "\n",
        "        with open(r\"out.csv\", mode='w') as file:\n",
        "          writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "          writer.writerow([\"Id\", \"ProbToYes\"])\n",
        "          for i in range(len(ypred)):\n",
        "            writer.writerow([int(i+1), ypred[i]])\n",
        "\n",
        "        ### Print AUC plot on train data\n",
        "\n",
        "        plot_auc_best (bst, x_train, y_train)\n",
        "\n",
        "        ### Print Shap plots\n",
        "\n",
        "        row_to_show = 15\n",
        "        data_for_prediction = x_train.iloc[row_to_show]\n",
        "        #data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
        "        #xgb.predict_proba(data_for_prediction_array)\n",
        "\n",
        "        # Create object that can calculate shap values\n",
        "        explainer = shap.TreeExplainer(bst)\n",
        "\n",
        "        # Calculate Shap values\n",
        "        shap_values = explainer.shap_values(data_for_prediction)\n",
        "\n",
        "        #shap.initjs()\n",
        "        shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7GJWvDYT8AP",
        "colab_type": "text"
      },
      "source": [
        "Choosing initial algorithm: RandomForest ot XGBOOST (in this case XGBOOST) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qc2a96I5ht7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0e029e3-d950-42b7-e1fe-9da5dcc14e4a"
      },
      "source": [
        "## We have initially used RandomForset for first level tuning and XGB for results improvement.\n",
        "\n",
        "### Hyper Params: We have used manual execution to define best setup of several HyperParams  ####\n",
        "### Hyper params validated on Random Forest\n",
        "### 1. threshold - the percentage of missing values for dropping a feature\n",
        "### 2. filling missing values strategy - 'most_frequent', 'mean', 'median'\n",
        "### 3. train\\test fraction\n",
        "\n",
        "### Hyper params validated on XGBOOST\n",
        "\n",
        "\n",
        "\n",
        "algorithm = \"XGB\"\n",
        "    \n",
        "if (algorithm == \"RF\"):\n",
        "  clf=RandomForestClassifier(n_estimators=450)\n",
        "  test(actual_train_set, actual_test_set, target_col, algorithm, clf, \"test\")\n",
        "elif (algorithm == \"XGB\"):\n",
        "  clf = xgb.XGBClassifier(seed=1,verbose_eval=True, n_jobs =-1, max_depth =7,learning_rate=0.3, reg_lambda =250, tree_method='gpu_hist')\n",
        "  test(actual_train_set, actual_test_set, target_col, algorithm, clf, \"test\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XGB execution\n",
            "Start cross validation\n",
            "[0]\ttrain-auc:0.679472+0.0128616\ttest-auc:0.665614+0.00695415\n",
            "[1]\ttrain-auc:0.695568+0.00716017\ttest-auc:0.685468+0.00790041\n",
            "[2]\ttrain-auc:0.71762+0.0149414\ttest-auc:0.702628+0.0237732\n",
            "[3]\ttrain-auc:0.748686+0.00507253\ttest-auc:0.730706+0.0126252\n",
            "[4]\ttrain-auc:0.766266+0.00535941\ttest-auc:0.743735+0.00946412\n",
            "[5]\ttrain-auc:0.775461+0.00345545\ttest-auc:0.749309+0.00974806\n",
            "[6]\ttrain-auc:0.78318+0.00346242\ttest-auc:0.755161+0.0101224\n",
            "[7]\ttrain-auc:0.789187+0.00373299\ttest-auc:0.760462+0.0102416\n",
            "[8]\ttrain-auc:0.794397+0.00439843\ttest-auc:0.76478+0.00902373\n",
            "[9]\ttrain-auc:0.800566+0.00301264\ttest-auc:0.769867+0.0107055\n",
            "[10]\ttrain-auc:0.804123+0.00363388\ttest-auc:0.77087+0.0104319\n",
            "[11]\ttrain-auc:0.80887+0.00429377\ttest-auc:0.774124+0.00836534\n",
            "[12]\ttrain-auc:0.812289+0.00439386\ttest-auc:0.776123+0.00820604\n",
            "[13]\ttrain-auc:0.815778+0.00443744\ttest-auc:0.778896+0.00814382\n",
            "[14]\ttrain-auc:0.81926+0.00525147\ttest-auc:0.780712+0.00659183\n",
            "[15]\ttrain-auc:0.821961+0.00487144\ttest-auc:0.781798+0.0068165\n",
            "[16]\ttrain-auc:0.825279+0.00324149\ttest-auc:0.784212+0.00885914\n",
            "[17]\ttrain-auc:0.827591+0.003309\ttest-auc:0.785531+0.00840268\n",
            "[18]\ttrain-auc:0.829798+0.00325544\ttest-auc:0.786423+0.00857763\n",
            "[19]\ttrain-auc:0.83211+0.00332791\ttest-auc:0.788179+0.00821565\n",
            "[20]\ttrain-auc:0.834257+0.00381045\ttest-auc:0.78935+0.00767525\n",
            "[21]\ttrain-auc:0.83614+0.00367654\ttest-auc:0.79138+0.00801362\n",
            "[22]\ttrain-auc:0.837981+0.00349125\ttest-auc:0.792383+0.00864553\n",
            "[23]\ttrain-auc:0.839832+0.00340989\ttest-auc:0.793464+0.00860298\n",
            "[24]\ttrain-auc:0.84127+0.00319819\ttest-auc:0.794208+0.0085505\n",
            "[25]\ttrain-auc:0.842881+0.00354324\ttest-auc:0.795028+0.00800689\n",
            "[26]\ttrain-auc:0.844233+0.00377842\ttest-auc:0.795276+0.00794191\n",
            "[27]\ttrain-auc:0.845758+0.00387009\ttest-auc:0.795641+0.00833942\n",
            "[28]\ttrain-auc:0.847164+0.00326051\ttest-auc:0.796806+0.00846715\n",
            "[29]\ttrain-auc:0.848543+0.00338235\ttest-auc:0.797846+0.00880562\n",
            "[30]\ttrain-auc:0.849592+0.00368741\ttest-auc:0.798427+0.00840744\n",
            "[31]\ttrain-auc:0.850838+0.00360204\ttest-auc:0.798867+0.00842887\n",
            "[32]\ttrain-auc:0.852033+0.00379502\ttest-auc:0.799144+0.00836734\n",
            "[33]\ttrain-auc:0.853296+0.00377297\ttest-auc:0.799568+0.00865575\n",
            "[34]\ttrain-auc:0.854058+0.00377342\ttest-auc:0.799697+0.00898843\n",
            "[35]\ttrain-auc:0.854835+0.00363252\ttest-auc:0.800264+0.00903932\n",
            "[36]\ttrain-auc:0.855899+0.0037364\ttest-auc:0.800488+0.00921759\n",
            "[37]\ttrain-auc:0.856826+0.00357998\ttest-auc:0.801254+0.00961496\n",
            "[38]\ttrain-auc:0.857836+0.00370628\ttest-auc:0.80135+0.0095496\n",
            "[39]\ttrain-auc:0.85871+0.00366999\ttest-auc:0.801099+0.00966096\n",
            "[40]\ttrain-auc:0.859697+0.00363119\ttest-auc:0.801404+0.00958465\n",
            "[41]\ttrain-auc:0.860824+0.00405187\ttest-auc:0.801981+0.00946619\n",
            "[42]\ttrain-auc:0.861745+0.00388887\ttest-auc:0.80228+0.00970481\n",
            "[43]\ttrain-auc:0.862653+0.00377811\ttest-auc:0.802172+0.00970863\n",
            "[44]\ttrain-auc:0.863575+0.00405475\ttest-auc:0.802048+0.00982318\n",
            "[45]\ttrain-auc:0.864459+0.0042898\ttest-auc:0.802409+0.0095147\n",
            "[46]\ttrain-auc:0.865472+0.00428999\ttest-auc:0.802782+0.00959262\n",
            "[47]\ttrain-auc:0.866185+0.00444466\ttest-auc:0.802814+0.00974604\n",
            "[48]\ttrain-auc:0.86686+0.00448717\ttest-auc:0.803034+0.00991515\n",
            "[49]\ttrain-auc:0.867569+0.00446059\ttest-auc:0.803097+0.0100286\n",
            "[50]\ttrain-auc:0.86827+0.00442081\ttest-auc:0.803156+0.00989926\n",
            "[51]\ttrain-auc:0.868796+0.00454652\ttest-auc:0.803405+0.00984809\n",
            "[52]\ttrain-auc:0.869559+0.00461811\ttest-auc:0.803644+0.00999718\n",
            "[53]\ttrain-auc:0.87053+0.00466788\ttest-auc:0.803887+0.0103582\n",
            "[54]\ttrain-auc:0.871293+0.00467384\ttest-auc:0.804031+0.0103964\n",
            "[55]\ttrain-auc:0.872243+0.00454972\ttest-auc:0.80424+0.0106212\n",
            "[56]\ttrain-auc:0.873165+0.0043955\ttest-auc:0.804557+0.0111287\n",
            "[57]\ttrain-auc:0.873911+0.00456608\ttest-auc:0.804752+0.0110975\n",
            "[58]\ttrain-auc:0.874711+0.00459695\ttest-auc:0.805089+0.0108329\n",
            "[59]\ttrain-auc:0.875466+0.00486211\ttest-auc:0.805161+0.0106804\n",
            "[60]\ttrain-auc:0.876383+0.00483296\ttest-auc:0.805515+0.0105821\n",
            "[61]\ttrain-auc:0.877055+0.00491754\ttest-auc:0.805479+0.010792\n",
            "[62]\ttrain-auc:0.877629+0.00510014\ttest-auc:0.805603+0.010808\n",
            "[63]\ttrain-auc:0.878153+0.00510599\ttest-auc:0.805859+0.0106012\n",
            "[64]\ttrain-auc:0.878859+0.00500533\ttest-auc:0.80609+0.0109693\n",
            "[65]\ttrain-auc:0.879387+0.00495184\ttest-auc:0.806192+0.0110417\n",
            "[66]\ttrain-auc:0.879851+0.0048502\ttest-auc:0.806194+0.0110465\n",
            "[67]\ttrain-auc:0.880458+0.00473638\ttest-auc:0.806503+0.0112589\n",
            "[68]\ttrain-auc:0.881093+0.00488158\ttest-auc:0.806785+0.0111254\n",
            "[69]\ttrain-auc:0.881703+0.0048913\ttest-auc:0.806732+0.0113144\n",
            "[70]\ttrain-auc:0.882533+0.00477826\ttest-auc:0.806825+0.0115321\n",
            "[71]\ttrain-auc:0.883241+0.00492481\ttest-auc:0.806907+0.0113497\n",
            "[72]\ttrain-auc:0.883623+0.00498552\ttest-auc:0.806826+0.0112816\n",
            "[73]\ttrain-auc:0.884163+0.00498099\ttest-auc:0.806811+0.0112134\n",
            "[74]\ttrain-auc:0.884712+0.00512599\ttest-auc:0.806839+0.010955\n",
            "[75]\ttrain-auc:0.885145+0.00498499\ttest-auc:0.807141+0.0112077\n",
            "[76]\ttrain-auc:0.885793+0.00499689\ttest-auc:0.807177+0.0115464\n",
            "[77]\ttrain-auc:0.886351+0.00521696\ttest-auc:0.807683+0.0109968\n",
            "[78]\ttrain-auc:0.886904+0.00529912\ttest-auc:0.807661+0.0109756\n",
            "[79]\ttrain-auc:0.887639+0.00523905\ttest-auc:0.807912+0.0108557\n",
            "[80]\ttrain-auc:0.888425+0.00523055\ttest-auc:0.80814+0.0110747\n",
            "[81]\ttrain-auc:0.889051+0.00520788\ttest-auc:0.808195+0.010985\n",
            "[82]\ttrain-auc:0.889651+0.00510025\ttest-auc:0.80809+0.0111074\n",
            "[83]\ttrain-auc:0.890213+0.00502947\ttest-auc:0.808135+0.0109903\n",
            "[84]\ttrain-auc:0.890749+0.00497448\ttest-auc:0.808205+0.0110857\n",
            "[85]\ttrain-auc:0.891118+0.00483019\ttest-auc:0.808463+0.0113061\n",
            "[86]\ttrain-auc:0.891518+0.00477182\ttest-auc:0.808469+0.011507\n",
            "[87]\ttrain-auc:0.891919+0.00480708\ttest-auc:0.808564+0.0115225\n",
            "[88]\ttrain-auc:0.892362+0.0048682\ttest-auc:0.80864+0.0118678\n",
            "[89]\ttrain-auc:0.892755+0.00485864\ttest-auc:0.808675+0.0119577\n",
            "[90]\ttrain-auc:0.893318+0.00493717\ttest-auc:0.808739+0.0121484\n",
            "[91]\ttrain-auc:0.893817+0.00491392\ttest-auc:0.808792+0.0122327\n",
            "[92]\ttrain-auc:0.894273+0.00491629\ttest-auc:0.808846+0.01229\n",
            "[93]\ttrain-auc:0.894581+0.0050731\ttest-auc:0.808683+0.0123959\n",
            "[94]\ttrain-auc:0.894962+0.00512661\ttest-auc:0.80873+0.0122574\n",
            "[95]\ttrain-auc:0.895547+0.00508214\ttest-auc:0.808668+0.0124637\n",
            "[96]\ttrain-auc:0.89622+0.00491728\ttest-auc:0.808895+0.01253\n",
            "[97]\ttrain-auc:0.896602+0.00491107\ttest-auc:0.808856+0.0126348\n",
            "[98]\ttrain-auc:0.896891+0.00493826\ttest-auc:0.808787+0.0127176\n",
            "[99]\ttrain-auc:0.897293+0.00489612\ttest-auc:0.809068+0.012524\n",
            "[100]\ttrain-auc:0.897683+0.00484235\ttest-auc:0.809015+0.0125009\n",
            "[101]\ttrain-auc:0.898076+0.00471899\ttest-auc:0.809131+0.0125791\n",
            "[102]\ttrain-auc:0.898555+0.00482056\ttest-auc:0.809441+0.0125017\n",
            "[103]\ttrain-auc:0.89888+0.00491288\ttest-auc:0.809413+0.0126799\n",
            "[104]\ttrain-auc:0.899325+0.0049613\ttest-auc:0.809375+0.0126082\n",
            "[105]\ttrain-auc:0.899721+0.0050161\ttest-auc:0.809379+0.0125072\n",
            "[106]\ttrain-auc:0.900139+0.00513865\ttest-auc:0.809284+0.0125179\n",
            "\n",
            "AUC plot of best result\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "AUC in train set: 0.9388321323747063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPNW_u5O9i7K",
        "colab_type": "text"
      },
      "source": [
        "*Mount* google drive:"
      ]
    }
  ]
}